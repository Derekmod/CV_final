\documentclass{article}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{bbm}
\usepackage{geometry}
\usepackage{mathtools}
\usepackage{amsthm}
\DeclareMathOperator*{\argmin}{arg\!\min}
\DeclareMathOperator*{\argmax}{arg\!\max}

\geometry{letterpaper, portrait, margin=1in}
\newcommand{\ul}[0]{\underline}
\newcommand{\hs}[1]{\hspace*{#1 cm}}
\newcommand{\ind}[0]{\indent}
\newcommand{\tx}[1]{\text{#1}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{remark}{Remark}


\title{Genre Classification of YouTube Videos}
\author{
  Modzelewski, Derek\\
  \texttt{dmodzel2@jhu.edu}
  \and
  Smillie, Dan\\
  \texttt{dsmilli1@jhu.edu}
}

\begin{document}

\maketitle

\tableofcontents

\section{Overview} %Dan

\section{Dataset} %Dan

\section{Methods} %Derek

\subsection{Models}

Google has published support for training Logistic multi-class classifiers for both frame-level and video-level features. Although this is a simple model, we expect it to be sufficient since the data is already preprocessed in a (hopefully) intelligent manner. It would be interesting to see if a shallow neural-network would perform better, but we didn't have the time to reverse-engineer Google's code and create our own model.

Since both frame-level and video-level features have the same (well-studied) model, any differences in the results should reflect the differences in aptitudes of the features, not the models.

\subsection{Training}

We used ? videos

\subsection{Evaluation}


\section{Results}

\section{Conclusions}

cite err_1410.csv - labels 6 and 15 are easier

training error goes down - requote results from slides

better at learning more common labels

frame level vs video features

\section{References}

cite YouTube 8M






%

\end{document}